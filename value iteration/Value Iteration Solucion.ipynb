{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration\n",
    "\n",
    "Vamos a ver como la programación dinamica se puede utilizar para derivar políticas óptimas para algunos ambientes. En este caso particular vamos a utilizar dos ambientes definidos en el libro de Sutton y Barto. \n",
    "\n",
    "Es recomendable leer el capítulo 4 del libro para entender a fondo todos los conceptos.\n",
    "\n",
    "## A entregar:\n",
    "\n",
    "- Notebook con el algoritmo de value iteration implementado (hecho en clase)\n",
    "- Comentarios sobre funciones implementadas en clase, que hacen y que es cada parametro.\n",
    "- Reporte con funciones de valor para diferentes tamaños de ambiente.\n",
    "- Reporte con graficas sobre el numero de iteraciones del algoritmo para diferentes tamaños de ambiente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definicion del ambiente\n",
    "\n",
    "En este caso vamos a usar como ambiente algo muy sencillo, Gridworld (del libro de Sutton y Barto). Basicamente se trata de una grilla rectangular de posibles posiciones donde los estados terminales se encuentran en la esquina superior izquierda ([0,0] en coordenadas) y en la esquina inferior derecha ([largo - 1, ancho - 1] en coordenadas). \n",
    "\n",
    "El objetivo es obtener la función de valor óptima para el entorno en el que estamos.\n",
    "\n",
    "Comenzamos definiendo el ambiente como una clase Python abajo, esto nos va a permitir crear varios ambientes de prueba para probar nuestro algoritmo y encontrar la función optima para todos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import io\n",
    "import sys\n",
    "import numpy as np\n",
    "from gym.envs.toy_text import discrete\n",
    "\n",
    "\n",
    "# Possible Actions\n",
    "UP = 0\n",
    "RIGHT = 1\n",
    "DOWN = 2\n",
    "LEFT = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridworldEnv(discrete.DiscreteEnv):\n",
    "    \"\"\"\n",
    "    ****\n",
    "    Forked from: https://github.com/dennybritz/reinforcement-learning/blob/master/lib/envs/gridworld.py\n",
    "    ****\n",
    "    Grid World environment from Sutton's Reinforcement Learning book chapter 4.\n",
    "    You are an agent on an MxN grid and your goal is to reach the terminal\n",
    "    state at the top left or the bottom right corner.\n",
    "\n",
    "    For example, a 4x4 grid looks as follows:\n",
    "\n",
    "    T  o  o  o\n",
    "    o  x  o  o\n",
    "    o  o  o  o\n",
    "    o  o  o  T\n",
    "\n",
    "    x is your position and T are the two terminal states.\n",
    "\n",
    "    You can take actions in each direction (UP=0, RIGHT=1, DOWN=2, LEFT=3).\n",
    "    Actions going off the edge leave you in your current state.\n",
    "    You receive a reward of -1 at each step until you reach a terminal state.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, shape=[4,4]):\n",
    "        if not isinstance(shape, (list, tuple)) or not len(shape) == 2:\n",
    "            raise ValueError('Shape argument must be a list/tuple of length 2')\n",
    "\n",
    "        self.shape = shape\n",
    "\n",
    "        number_states = np.prod(shape)\n",
    "        number_actions = 4\n",
    "        \n",
    "        self.number_actions = number_actions\n",
    "        self.number_states = number_states\n",
    "\n",
    "        MAX_Y = shape[0]\n",
    "        MAX_X = shape[1]\n",
    "\n",
    "        transition_probabilities = {}\n",
    "        grid = np.arange(number_states).reshape(shape)\n",
    "        it = np.nditer(grid, flags=['multi_index'])\n",
    "\n",
    "        while not it.finished:\n",
    "            s = it.iterindex\n",
    "            y, x = it.multi_index\n",
    "\n",
    "            transition_probabilities[s] = {a : [] for a in range(number_actions)}\n",
    "\n",
    "            is_done = lambda s: s == 0 or s == (number_states - 1)\n",
    "            reward = 0.0 if is_done(s) else -1.0\n",
    "\n",
    "            # We're stuck in a terminal state\n",
    "            if is_done(s):\n",
    "                transition_probabilities[s][UP] = [(1.0, s, reward, True)]\n",
    "                transition_probabilities[s][RIGHT] = [(1.0, s, reward, True)]\n",
    "                transition_probabilities[s][DOWN] = [(1.0, s, reward, True)]\n",
    "                transition_probabilities[s][LEFT] = [(1.0, s, reward, True)]\n",
    "            # Not a terminal state\n",
    "            else:\n",
    "                ns_up = s if y == 0 else s - MAX_X\n",
    "                ns_right = s if x == (MAX_X - 1) else s + 1\n",
    "                ns_down = s if y == (MAX_Y - 1) else s + MAX_X\n",
    "                ns_left = s if x == 0 else s - 1\n",
    "                transition_probabilities[s][UP] = [(1.0, ns_up, reward, is_done(ns_up))]\n",
    "                transition_probabilities[s][RIGHT] = [(1.0, ns_right, reward, is_done(ns_right))]\n",
    "                transition_probabilities[s][DOWN] = [(1.0, ns_down, reward, is_done(ns_down))]\n",
    "                transition_probabilities[s][LEFT] = [(1.0, ns_left, reward, is_done(ns_left))]\n",
    "\n",
    "            it.iternext()\n",
    "\n",
    "        # Initial state distribution is uniform\n",
    "        isd = np.ones(number_states) / number_states\n",
    "\n",
    "        # We expose the model of the environment for educational purposes\n",
    "        # This should not be used in any model-free learning algorithm\n",
    "        self.transition_probabilities = transition_probabilities\n",
    "\n",
    "        super(GridworldEnv, self).__init__(number_states, number_actions, transition_probabilities, isd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicializando una nueva instancia del ambiente\n",
    "\n",
    "En la celda de abajo vamos a crear una nueva instancia del ambiente definido arriba. En este caso, y por ser un ejemplo, conocemos la funcion de transición del ambiente (llamada `transition_probabilities`). Esta función nos dice, para cada estado, el resultado de tomar cada accion posible.\n",
    "\n",
    "*** \n",
    "\n",
    "Esta funcion esta representada por tuplas de la forma: `(Probabilidad, estado_siguiente, recompensa, estado_final)`. \n",
    "\n",
    "- Al ser un ambiente deterministico las **probabilidades** son siempre 1 (al tomar una accion tenemos 100% de chances de caer en el siguiente estado, otros casos una acción podría tener una distribuccion de probabilidades sobre todos los estados posibles).\n",
    "\n",
    "- Los **estados_siguientes** apuntan a dónde mirar en la funcion de transición.\n",
    "\n",
    "- Las **recompensas** como se mencionó arriba son siempre -1 por cada movimiento, excepto cuando estamos en un estado final.\n",
    "\n",
    "- El **estad_final** es un flag booleano (Verdadero o Falso) que nos dice si el estado al que nos movemos es final o no.\n",
    "\n",
    "***\n",
    "\n",
    "Por ejemplo: si estamos en el estado 0, tomar cualquier accion tiene recompensa 0, con probabilidad 100% (es un ambiente determinístico) y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: [(1.0, 0, 0.0, True)],\n",
       "  1: [(1.0, 0, 0.0, True)],\n",
       "  2: [(1.0, 0, 0.0, True)],\n",
       "  3: [(1.0, 0, 0.0, True)]},\n",
       " 1: {0: [(1.0, 1, -1.0, False)],\n",
       "  1: [(1.0, 2, -1.0, False)],\n",
       "  2: [(1.0, 5, -1.0, False)],\n",
       "  3: [(1.0, 0, -1.0, True)]},\n",
       " 2: {0: [(1.0, 2, -1.0, False)],\n",
       "  1: [(1.0, 3, -1.0, False)],\n",
       "  2: [(1.0, 6, -1.0, False)],\n",
       "  3: [(1.0, 1, -1.0, False)]},\n",
       " 3: {0: [(1.0, 3, -1.0, False)],\n",
       "  1: [(1.0, 3, -1.0, False)],\n",
       "  2: [(1.0, 7, -1.0, False)],\n",
       "  3: [(1.0, 2, -1.0, False)]},\n",
       " 4: {0: [(1.0, 0, -1.0, True)],\n",
       "  1: [(1.0, 5, -1.0, False)],\n",
       "  2: [(1.0, 8, -1.0, False)],\n",
       "  3: [(1.0, 4, -1.0, False)]},\n",
       " 5: {0: [(1.0, 1, -1.0, False)],\n",
       "  1: [(1.0, 6, -1.0, False)],\n",
       "  2: [(1.0, 9, -1.0, False)],\n",
       "  3: [(1.0, 4, -1.0, False)]},\n",
       " 6: {0: [(1.0, 2, -1.0, False)],\n",
       "  1: [(1.0, 7, -1.0, False)],\n",
       "  2: [(1.0, 10, -1.0, False)],\n",
       "  3: [(1.0, 5, -1.0, False)]},\n",
       " 7: {0: [(1.0, 3, -1.0, False)],\n",
       "  1: [(1.0, 7, -1.0, False)],\n",
       "  2: [(1.0, 11, -1.0, False)],\n",
       "  3: [(1.0, 6, -1.0, False)]},\n",
       " 8: {0: [(1.0, 4, -1.0, False)],\n",
       "  1: [(1.0, 9, -1.0, False)],\n",
       "  2: [(1.0, 12, -1.0, False)],\n",
       "  3: [(1.0, 8, -1.0, False)]},\n",
       " 9: {0: [(1.0, 5, -1.0, False)],\n",
       "  1: [(1.0, 10, -1.0, False)],\n",
       "  2: [(1.0, 13, -1.0, False)],\n",
       "  3: [(1.0, 8, -1.0, False)]},\n",
       " 10: {0: [(1.0, 6, -1.0, False)],\n",
       "  1: [(1.0, 11, -1.0, False)],\n",
       "  2: [(1.0, 14, -1.0, False)],\n",
       "  3: [(1.0, 9, -1.0, False)]},\n",
       " 11: {0: [(1.0, 7, -1.0, False)],\n",
       "  1: [(1.0, 11, -1.0, False)],\n",
       "  2: [(1.0, 15, -1.0, True)],\n",
       "  3: [(1.0, 10, -1.0, False)]},\n",
       " 12: {0: [(1.0, 8, -1.0, False)],\n",
       "  1: [(1.0, 13, -1.0, False)],\n",
       "  2: [(1.0, 12, -1.0, False)],\n",
       "  3: [(1.0, 12, -1.0, False)]},\n",
       " 13: {0: [(1.0, 9, -1.0, False)],\n",
       "  1: [(1.0, 14, -1.0, False)],\n",
       "  2: [(1.0, 13, -1.0, False)],\n",
       "  3: [(1.0, 12, -1.0, False)]},\n",
       " 14: {0: [(1.0, 10, -1.0, False)],\n",
       "  1: [(1.0, 15, -1.0, True)],\n",
       "  2: [(1.0, 14, -1.0, False)],\n",
       "  3: [(1.0, 13, -1.0, False)]},\n",
       " 15: {0: [(1.0, 15, 0.0, True)],\n",
       "  1: [(1.0, 15, 0.0, True)],\n",
       "  2: [(1.0, 15, 0.0, True)],\n",
       "  3: [(1.0, 15, 0.0, True)]}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = GridworldEnv()   # Create a new gridworld object\n",
    "grid.transition_probabilities\n",
    "\n",
    "# Recordar: \n",
    "# UP = 0\n",
    "# RIGHT = 1\n",
    "# DOWN = 2\n",
    "# LEFT = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recordando Value Iteration del libro de Sutton y Barto:\n",
    "\n",
    "![Image](https://i.imgur.com/PBtFBCG.png)\n",
    "\n",
    "\n",
    "Este es el algoritmo que vamos a implementar en las celdas siguientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_action_values(env, state, V, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Helper function to calculate the value for all actions in a given state.\n",
    "    \n",
    "    Args:\n",
    "        env: openAI.gym environment\n",
    "        state: The state to consider (int)\n",
    "        V: The value to use as an estimator, Vector of length env.number_states\n",
    "    \n",
    "    Returns:\n",
    "        A vector of length env.number_actions containing the expected value of each action.\n",
    "    \"\"\"\n",
    "    action_values = np.zeros(env.number_actions)\n",
    "    \n",
    "    for a in range(len(action_values)):\n",
    "        for prob, next_state, reward, done in env.transition_probabilities[state][a]:\n",
    "            action_values[a] += prob * (reward + discount_factor * V[next_state])\n",
    "    \n",
    "#     print(f\"Action values for state {state} \\n {action_values}\")\n",
    "    \n",
    "    return action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, theta=0.0001, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Value Iteration Algorithm.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI env. env.transition_probabilities represents the transition probabilities of the environment.\n",
    "            env.transition_probabilities[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.number_states is a number of states in the environment. \n",
    "            env.number_actions is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V) of the optimal policy and the optimal value function.\n",
    "    \"\"\"\n",
    "    V = np.zeros(env.number_states)\n",
    "    \n",
    "    counter = 0    # We want to count how many times we compute the action values (number of iterations of the algorithm)\n",
    "    \n",
    "    while True:\n",
    "        # Stopping condition\n",
    "        delta = 0\n",
    "        \n",
    "        # Update each state...\n",
    "        for s in range(env.number_states):\n",
    "            counter += 1  # Update the visit counter\n",
    "            \n",
    "            # Do a one-step lookahead to find the best action\n",
    "            A = compute_action_values(env, s, V, discount_factor)\n",
    "            best_action_value = np.max(A)\n",
    "        \n",
    "            # Calculate delta across all states seen so far\n",
    "            delta = max(delta, np.abs(best_action_value - V[s]))\n",
    "            \n",
    "            # Update the value function. Ref: Sutton book eq. 4.10. \n",
    "            V[s] = best_action_value        \n",
    "        \n",
    "        # Check if we can stop \n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    # Create a deterministic policy using the optimal value function\n",
    "    policy = np.zeros([env.number_states, env.number_actions])\n",
    "    for s in range(env.number_states):\n",
    "        # One step lookahead to find the best action for this state\n",
    "        A = compute_action_values(env, s, V, discount_factor)\n",
    "        best_action = np.argmax(A)\n",
    "        # Always take the best action\n",
    "        policy[s, best_action] = 1.0\n",
    "    \n",
    "    return policy, V, counter # also need to return the counter so we can plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action values for state 0 \n",
      " [0. 0. 0. 0.]\n",
      "Action values for state 1 \n",
      " [-1. -1. -1. -1.]\n",
      "Action values for state 2 \n",
      " [-1. -1. -1. -2.]\n",
      "Action values for state 3 \n",
      " [-1. -1. -1. -2.]\n",
      "Action values for state 4 \n",
      " [-1. -1. -1. -1.]\n",
      "Action values for state 5 \n",
      " [-2. -1. -1. -2.]\n",
      "Action values for state 6 \n",
      " [-2. -1. -1. -2.]\n",
      "Action values for state 7 \n",
      " [-2. -1. -1. -2.]\n",
      "Action values for state 8 \n",
      " [-2. -1. -1. -1.]\n",
      "Action values for state 9 \n",
      " [-2. -1. -1. -2.]\n",
      "Action values for state 10 \n",
      " [-2. -1. -1. -2.]\n",
      "Action values for state 11 \n",
      " [-2. -1. -1. -2.]\n",
      "Action values for state 12 \n",
      " [-2. -1. -1. -1.]\n",
      "Action values for state 13 \n",
      " [-2. -1. -1. -2.]\n",
      "Action values for state 14 \n",
      " [-2. -1. -1. -2.]\n",
      "Action values for state 15 \n",
      " [0. 0. 0. 0.]\n",
      "Action values for state 0 \n",
      " [0. 0. 0. 0.]\n",
      "Action values for state 1 \n",
      " [-2. -2. -2. -1.]\n",
      "Action values for state 2 \n",
      " [-2. -2. -2. -2.]\n",
      "Action values for state 3 \n",
      " [-2. -2. -2. -3.]\n",
      "Action values for state 4 \n",
      " [-1. -2. -2. -2.]\n",
      "Action values for state 5 \n",
      " [-2. -2. -2. -2.]\n",
      "Action values for state 6 \n",
      " [-3. -2. -2. -3.]\n",
      "Action values for state 7 \n",
      " [-3. -2. -2. -3.]\n",
      "Action values for state 8 \n",
      " [-2. -2. -2. -2.]\n",
      "Action values for state 9 \n",
      " [-3. -2. -2. -3.]\n",
      "Action values for state 10 \n",
      " [-3. -2. -2. -3.]\n",
      "Action values for state 11 \n",
      " [-3. -2. -1. -3.]\n",
      "Action values for state 12 \n",
      " [-3. -2. -2. -2.]\n",
      "Action values for state 13 \n",
      " [-3. -2. -2. -3.]\n",
      "Action values for state 14 \n",
      " [-3. -1. -2. -3.]\n",
      "Action values for state 15 \n",
      " [0. 0. 0. 0.]\n",
      "Action values for state 0 \n",
      " [0. 0. 0. 0.]\n",
      "Action values for state 1 \n",
      " [-2. -3. -3. -1.]\n",
      "Action values for state 2 \n",
      " [-3. -3. -3. -2.]\n",
      "Action values for state 3 \n",
      " [-3. -3. -3. -3.]\n",
      "Action values for state 4 \n",
      " [-1. -3. -3. -2.]\n",
      "Action values for state 5 \n",
      " [-2. -3. -3. -2.]\n",
      "Action values for state 6 \n",
      " [-3. -3. -3. -3.]\n",
      "Action values for state 7 \n",
      " [-4. -3. -2. -4.]\n",
      "Action values for state 8 \n",
      " [-2. -3. -3. -3.]\n",
      "Action values for state 9 \n",
      " [-3. -3. -3. -3.]\n",
      "Action values for state 10 \n",
      " [-4. -2. -2. -4.]\n",
      "Action values for state 11 \n",
      " [-3. -2. -1. -3.]\n",
      "Action values for state 12 \n",
      " [-3. -3. -3. -3.]\n",
      "Action values for state 13 \n",
      " [-4. -2. -3. -4.]\n",
      "Action values for state 14 \n",
      " [-3. -1. -2. -3.]\n",
      "Action values for state 15 \n",
      " [0. 0. 0. 0.]\n",
      "Action values for state 0 \n",
      " [0. 0. 0. 0.]\n",
      "Action values for state 1 \n",
      " [-2. -3. -3. -1.]\n",
      "Action values for state 2 \n",
      " [-3. -4. -4. -2.]\n",
      "Action values for state 3 \n",
      " [-4. -4. -3. -3.]\n",
      "Action values for state 4 \n",
      " [-1. -3. -3. -2.]\n",
      "Action values for state 5 \n",
      " [-2. -4. -4. -2.]\n",
      "Action values for state 6 \n",
      " [-3. -3. -3. -3.]\n",
      "Action values for state 7 \n",
      " [-4. -3. -2. -4.]\n",
      "Action values for state 8 \n",
      " [-2. -4. -4. -3.]\n",
      "Action values for state 9 \n",
      " [-3. -3. -3. -3.]\n",
      "Action values for state 10 \n",
      " [-4. -2. -2. -4.]\n",
      "Action values for state 11 \n",
      " [-3. -2. -1. -3.]\n",
      "Action values for state 12 \n",
      " [-3. -3. -4. -4.]\n",
      "Action values for state 13 \n",
      " [-4. -2. -3. -4.]\n",
      "Action values for state 14 \n",
      " [-3. -1. -2. -3.]\n",
      "Action values for state 15 \n",
      " [0. 0. 0. 0.]\n",
      "Action values for state 0 \n",
      " [0. 0. 0. 0.]\n",
      "Action values for state 1 \n",
      " [-2. -3. -3. -1.]\n",
      "Action values for state 2 \n",
      " [-3. -4. -4. -2.]\n",
      "Action values for state 3 \n",
      " [-4. -4. -3. -3.]\n",
      "Action values for state 4 \n",
      " [-1. -3. -3. -2.]\n",
      "Action values for state 5 \n",
      " [-2. -4. -4. -2.]\n",
      "Action values for state 6 \n",
      " [-3. -3. -3. -3.]\n",
      "Action values for state 7 \n",
      " [-4. -3. -2. -4.]\n",
      "Action values for state 8 \n",
      " [-2. -4. -4. -3.]\n",
      "Action values for state 9 \n",
      " [-3. -3. -3. -3.]\n",
      "Action values for state 10 \n",
      " [-4. -2. -2. -4.]\n",
      "Action values for state 11 \n",
      " [-3. -2. -1. -3.]\n",
      "Action values for state 12 \n",
      " [-3. -3. -4. -4.]\n",
      "Action values for state 13 \n",
      " [-4. -2. -3. -4.]\n",
      "Action values for state 14 \n",
      " [-3. -1. -2. -3.]\n",
      "Action values for state 15 \n",
      " [0. 0. 0. 0.]\n",
      "For env size 4 the number of iterations was 64\n"
     ]
    }
   ],
   "source": [
    "env = GridworldEnv()\n",
    "policy, v, count = value_iteration(env)   # get the count as well\n",
    "\n",
    "print(f\"For env size 4 the number of iterations was {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Probability Distribution:\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\n",
      "[[0 3 3 2]\n",
      " [0 0 0 2]\n",
      " [0 0 1 2]\n",
      " [0 1 1 0]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function:\n",
      "[ 0. -1. -2. -3. -1. -2. -3. -2. -2. -3. -2. -1. -3. -2. -1.  0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Value Function:\")\n",
    "print(v)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algunas funciones de valor:\n",
      "Ambiente 5x5: [ 0. -1. -2. -3. -4. -5. -1. -2. -3. -4. -5. -4. -2. -3. -4. -5. -4. -3.\n",
      " -3. -4. -5. -4. -3. -2. -4. -5. -4. -3. -2. -1. -5. -4. -3. -2. -1.  0.]\n",
      "Ambiente 25x25: [  0.  -1.  -2.  -3.  -4.  -5.  -6.  -7.  -8.  -9. -10. -11. -12. -13.\n",
      " -14. -15. -16. -17. -18. -19. -20. -21. -22. -23. -24. -25.  -1.  -2.\n",
      "  -3.  -4.  -5.  -6.  -7.  -8.  -9. -10. -11. -12. -13. -14. -15. -16.\n",
      " -17. -18. -19. -20. -21. -22. -23. -24. -25. -24.  -2.  -3.  -4.  -5.\n",
      "  -6.  -7.  -8.  -9. -10. -11. -12. -13. -14. -15. -16. -17. -18. -19.\n",
      " -20. -21. -22. -23. -24. -25. -24. -23.  -3.  -4.  -5.  -6.  -7.  -8.\n",
      "  -9. -10. -11. -12. -13. -14. -15. -16. -17. -18. -19. -20. -21. -22.\n",
      " -23. -24. -25. -24. -23. -22.  -4.  -5.  -6.  -7.  -8.  -9. -10. -11.\n",
      " -12. -13. -14. -15. -16. -17. -18. -19. -20. -21. -22. -23. -24. -25.\n",
      " -24. -23. -22. -21.  -5.  -6.  -7.  -8.  -9. -10. -11. -12. -13. -14.\n",
      " -15. -16. -17. -18. -19. -20. -21. -22. -23. -24. -25. -24. -23. -22.\n",
      " -21. -20.  -6.  -7.  -8.  -9. -10. -11. -12. -13. -14. -15. -16. -17.\n",
      " -18. -19. -20. -21. -22. -23. -24. -25. -24. -23. -22. -21. -20. -19.\n",
      "  -7.  -8.  -9. -10. -11. -12. -13. -14. -15. -16. -17. -18. -19. -20.\n",
      " -21. -22. -23. -24. -25. -24. -23. -22. -21. -20. -19. -18.  -8.  -9.\n",
      " -10. -11. -12. -13. -14. -15. -16. -17. -18. -19. -20. -21. -22. -23.\n",
      " -24. -25. -24. -23. -22. -21. -20. -19. -18. -17.  -9. -10. -11. -12.\n",
      " -13. -14. -15. -16. -17. -18. -19. -20. -21. -22. -23. -24. -25. -24.\n",
      " -23. -22. -21. -20. -19. -18. -17. -16. -10. -11. -12. -13. -14. -15.\n",
      " -16. -17. -18. -19. -20. -21. -22. -23. -24. -25. -24. -23. -22. -21.\n",
      " -20. -19. -18. -17. -16. -15. -11. -12. -13. -14. -15. -16. -17. -18.\n",
      " -19. -20. -21. -22. -23. -24. -25. -24. -23. -22. -21. -20. -19. -18.\n",
      " -17. -16. -15. -14. -12. -13. -14. -15. -16. -17. -18. -19. -20. -21.\n",
      " -22. -23. -24. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15.\n",
      " -14. -13. -13. -14. -15. -16. -17. -18. -19. -20. -21. -22. -23. -24.\n",
      " -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14. -13. -12.\n",
      " -14. -15. -16. -17. -18. -19. -20. -21. -22. -23. -24. -25. -24. -23.\n",
      " -22. -21. -20. -19. -18. -17. -16. -15. -14. -13. -12. -11. -15. -16.\n",
      " -17. -18. -19. -20. -21. -22. -23. -24. -25. -24. -23. -22. -21. -20.\n",
      " -19. -18. -17. -16. -15. -14. -13. -12. -11. -10. -16. -17. -18. -19.\n",
      " -20. -21. -22. -23. -24. -25. -24. -23. -22. -21. -20. -19. -18. -17.\n",
      " -16. -15. -14. -13. -12. -11. -10.  -9. -17. -18. -19. -20. -21. -22.\n",
      " -23. -24. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14.\n",
      " -13. -12. -11. -10.  -9.  -8. -18. -19. -20. -21. -22. -23. -24. -25.\n",
      " -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14. -13. -12. -11.\n",
      " -10.  -9.  -8.  -7. -19. -20. -21. -22. -23. -24. -25. -24. -23. -22.\n",
      " -21. -20. -19. -18. -17. -16. -15. -14. -13. -12. -11. -10.  -9.  -8.\n",
      "  -7.  -6. -20. -21. -22. -23. -24. -25. -24. -23. -22. -21. -20. -19.\n",
      " -18. -17. -16. -15. -14. -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.\n",
      " -21. -22. -23. -24. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16.\n",
      " -15. -14. -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4. -22. -23.\n",
      " -24. -25. -24. -23. -22. -21. -20. -19. -18. -17. -16. -15. -14. -13.\n",
      " -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.  -3. -23. -24. -25. -24.\n",
      " -23. -22. -21. -20. -19. -18. -17. -16. -15. -14. -13. -12. -11. -10.\n",
      "  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2. -24. -25. -24. -23. -22. -21.\n",
      " -20. -19. -18. -17. -16. -15. -14. -13. -12. -11. -10.  -9.  -8.  -7.\n",
      "  -6.  -5.  -4.  -3.  -2.  -1. -25. -24. -23. -22. -21. -20. -19. -18.\n",
      " -17. -16. -15. -14. -13. -12. -11. -10.  -9.  -8.  -7.  -6.  -5.  -4.\n",
      "  -3.  -2.  -1.   0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Solution\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "env_sizes = range(1, 50)  # Explore some size range  \n",
    "counts = []\n",
    "value_functions = []\n",
    "\n",
    "for size in env_sizes:\n",
    "    env = GridworldEnv(shape=[size, size])\n",
    "    policy, v, count = value_iteration(env)\n",
    "    counts.append(count)\n",
    "    value_functions.append(v)\n",
    "\n",
    "    \n",
    "print(\"Algunas funciones de valor:\")\n",
    "print(f\"Ambiente 5x5: {value_functions[5]}\")\n",
    "print(f\"Ambiente 25x25: {value_functions[25]}\")\n",
    "\n",
    "\n",
    "plt.plot(env_sizes, counts)\n",
    "plt.xlabel(\"Tamaño de ambiente\")\n",
    "plt.ylabel(\"Iteraciones\")\n",
    "plt.title(\"Iteraciones para distintos tamaños de ambiente\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es importante observar el crecimiento exponencial del número de iteraciones en funcion al tamaño del ambiente. Eventualmente el ambiente va a ser demasiado grande como para tratar de estimar una politica óptima usando value iteration, este es uno de los mayores problemas del algortmo en la practica."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
